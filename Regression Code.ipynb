{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the project were a regression problem, the following would be sample code. Based off of Kaggle modelling house prices with regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORT AND SET UP DATA**\n",
    "\n",
    "The following cell imports and reads the data, setting it up in a comprehensible format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>2.484400e+04</td>\n",
       "      <td>24844.000000</td>\n",
       "      <td>24844.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.040211</td>\n",
       "      <td>36.845979</td>\n",
       "      <td>1.288118</td>\n",
       "      <td>1.564845</td>\n",
       "      <td>15.765939</td>\n",
       "      <td>7.881702</td>\n",
       "      <td>72.928192</td>\n",
       "      <td>3.200935</td>\n",
       "      <td>736.620633</td>\n",
       "      <td>951.564281</td>\n",
       "      <td>2.253277e+03</td>\n",
       "      <td>1.953510</td>\n",
       "      <td>0.339961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.196458</td>\n",
       "      <td>13.241031</td>\n",
       "      <td>0.453162</td>\n",
       "      <td>0.496761</td>\n",
       "      <td>26.337659</td>\n",
       "      <td>18.785623</td>\n",
       "      <td>40.728075</td>\n",
       "      <td>6.440581</td>\n",
       "      <td>292.545306</td>\n",
       "      <td>749.563452</td>\n",
       "      <td>1.403404e+04</td>\n",
       "      <td>0.642311</td>\n",
       "      <td>0.473705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.676712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.920548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>743.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.500000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.178082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>1271.000000</td>\n",
       "      <td>1.200000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.328767</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.317808</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>1624.000000</td>\n",
       "      <td>2.050000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.476712</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>56.356164</td>\n",
       "      <td>1138.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Label             b             c             d             e  \\\n",
       "count  24844.000000  24844.000000  24844.000000  24844.000000  24844.000000   \n",
       "mean       0.040211     36.845979      1.288118      1.564845     15.765939   \n",
       "std        0.196458     13.241031      0.453162      0.496761     26.337659   \n",
       "min        0.000000     18.676712      0.000000      0.000000      0.000000   \n",
       "25%        0.000000     25.920548      1.000000      1.000000      0.000000   \n",
       "50%        0.000000     34.178082      1.000000      2.000000      8.000000   \n",
       "75%        0.000000     45.328767      2.000000      2.000000      8.000000   \n",
       "max        1.000000     95.476712      2.000000      2.000000     81.000000   \n",
       "\n",
       "                  f             g             h             i             j  \\\n",
       "count  24844.000000  24844.000000  24844.000000  24844.000000  24844.000000   \n",
       "mean       7.881702     72.928192      3.200935    736.620633    951.564281   \n",
       "std       18.785623     40.728075      6.440581    292.545306    749.563452   \n",
       "min        1.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "25%        2.000000     27.000000      0.000000    743.000000      0.000000   \n",
       "50%        2.000000     99.000000      0.000000    826.000000   1271.000000   \n",
       "75%        8.000000     99.000000      3.317808    904.000000   1624.000000   \n",
       "max       99.000000     99.000000     56.356164   1138.000000   1900.000000   \n",
       "\n",
       "                  k             l             m  \n",
       "count  2.484400e+04  24844.000000  24844.000000  \n",
       "mean   2.253277e+03      1.953510      0.339961  \n",
       "std    1.403404e+04      0.642311      0.473705  \n",
       "min    0.000000e+00      1.000000      0.000000  \n",
       "25%    8.500000e+02      2.000000      0.000000  \n",
       "50%    1.200000e+03      2.000000      0.000000  \n",
       "75%    2.050000e+03      2.000000      1.000000  \n",
       "max    2.000000e+06      3.000000      1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to access the location of the directory you are working in (the users/ameliebuc thing):\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "spreadsheet_file_path = \"/Users/ameliebuc/Documents/byond_internship/ImBlanced-Classification.csv\"\n",
    "data = pd.read_csv(spreadsheet_file_path, encoding = 'utf-8')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle interpets the data descriptions in the leftmost column as follows: (This is in terms of their \"Melbourne Housing\" dataset. https://www.kaggle.com/dansbecker/explore-your-data)\n",
    "\n",
    "The results show 8 numbers for each column in your original dataset. The first number, the **count**, shows how many rows have non-missing values.\n",
    "\n",
    "**Missing values** arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house. We'll come back to the topic of missing data.\n",
    "\n",
    "The second value is the **mean**, which is the average. Under that, **std** is the standard deviation, which measures how numerically spread out the values are.\n",
    "\n",
    "To interpret the **min, 25%, 50%, 75% and max** values, imagine sorting each column from lowest to highest value. The first (smallest) value is the min. If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values. That is the 25% value (pronounced \"25th percentile\"). The 50th and 75th percentiles are defined analgously, and the max is the largest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "# dropna drops missing values (think of na as \"not available\")\n",
    "# data = data.dropna(axis=0)\n",
    "y = data.Label\n",
    "data_features = ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']\n",
    "X = data[data_features]\n",
    "#X.describe()\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUILD MODEL: SELECT DATA AND FEATURES FOR MODELING**\n",
    "\n",
    "1. Choose variables/columns for modelling manually.\n",
    "2. To select a prediction target (the column we want to predict), use dot notation. By convention, the prediction target is called **y**.\n",
    "3. Choose a few features to later predict the Label (by convention called X).\n",
    "\n",
    "Define, fit, predict, evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  0\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  0\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  0\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# split data into a set for training and for validation based on a random number generator\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# check what the best depth for the decision tree should be\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    data_model = DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes, random_state = 1)\n",
    "    data_model.fit(train_X, train_y)\n",
    "    val_predictions = data_model.predict(val_X)\n",
    "    # Mean absolute error: on average, model is off by about X. error = |actual-predicted|\n",
    "    val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "    return val_mae\n",
    "# print(\"Validation Set's MAE: {}\".format(val_mae))\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n",
    "\n",
    "This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.\n",
    "\n",
    "At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.\n",
    "\n",
    "Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. \n",
    "\n",
    "There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "\n",
    "**Overfitting**: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n",
    "\n",
    "**Underfitting**: failing to capture relevant patterns, again leading to less accurate predictions.\n",
    "\n",
    "**We use validation data, which isn't used in model training**, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
